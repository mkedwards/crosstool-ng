From 07d8e4e144372f356db43457f960b7760fdccf45 Mon Sep 17 00:00:00 2001
From: michaedw in build chroot <build@ctbu-bld5.cisco.com>
Date: Thu, 30 Jun 2011 22:08:41 +0000
Subject: [PATCH] Implement jsimd_idct_ifast() using NEON intrinsics

---
 jddctmgr.c       |   18 ++-
 simd/jsimd.h     |    1 +
 simd/jsimd_arm.c |  519 +++++++++++++++++++++++++++++++++++++++++++++++++++++-
 turbojpeg.c      |    1 +
 4 files changed, 524 insertions(+), 15 deletions(-)

diff --git a/jddctmgr.c b/jddctmgr.c
index 044e469..6b9063c 100644
--- a/jddctmgr.c
+++ b/jddctmgr.c
@@ -93,7 +93,7 @@ METHODDEF(void)
 start_pass (j_decompress_ptr cinfo)
 {
   my_idct_ptr idct = (my_idct_ptr) cinfo->idct;
-  int ci, i;
+  int ci, i, can;
   jpeg_component_info *compptr;
   int method = 0;
   inverse_DCT_method_ptr method_ptr = NULL;
@@ -109,18 +109,22 @@ start_pass (j_decompress_ptr cinfo)
       method = JDCT_ISLOW;	/* jidctred uses islow-style table */
       break;
     case 2:
-      if (jsimd_can_idct_2x2())
+      method = JDCT_ISLOW;	/* jidctred uses islow-style table */
+      if (can = jsimd_can_idct_2x2()) {
         method_ptr = jsimd_idct_2x2;
-      else
+        if (can == -1)
+          method = JDCT_IFAST;
+      } else
         method_ptr = jpeg_idct_2x2;
-      method = JDCT_ISLOW;	/* jidctred uses islow-style table */
       break;
     case 4:
-      if (jsimd_can_idct_4x4())
+      method = JDCT_ISLOW;	/* jidctred uses islow-style table */
+      if (can = jsimd_can_idct_4x4()) {
         method_ptr = jsimd_idct_4x4;
-      else
+        if (can == -1)
+          method = JDCT_IFAST;
+      } else
         method_ptr = jpeg_idct_4x4;
-      method = JDCT_ISLOW;	/* jidctred uses islow-style table */
       break;
 #endif
     case DCTSIZE:
diff --git a/simd/jsimd.h b/simd/jsimd.h
index 39a0867..645c087 100644
--- a/simd/jsimd.h
+++ b/simd/jsimd.h
@@ -18,6 +18,7 @@
 #define JSIMD_SSE        0x04
 #define JSIMD_SSE2       0x08
 #define JSIMD_ARM_NEON   0x10
+#define JSIMD_ARM_NEON_ASM 0x20
 
 /* Short forms of external names for systems with brain-damaged linkers. */
 
diff --git a/simd/jsimd_arm.c b/simd/jsimd_arm.c
index 1a5cdd3..51fa4fa 100644
--- a/simd/jsimd_arm.c
+++ b/simd/jsimd_arm.c
@@ -15,6 +15,8 @@
  * Based on the stubs from 'jsimd_none.c'
  */
 
+#include <arm_neon.h>
+
 #define JPEG_INTERNALS
 #include "../jinclude.h"
 #include "../jpeglib.h"
@@ -27,6 +29,291 @@
 #include <string.h>
 #include <ctype.h>
 
+//#define DEBUG_IDCT_DEQUANTIZE
+//#define DEBUG_IDCT_TRANSFORM
+//#define DEBUG_IDCT_TRANSPOSE
+//#define DEBUG_IDCT_NARROW
+
+typedef struct {
+  int16x8x4_t half[2];
+} int16_8x8_t;
+
+inline static int16_8x8_t dq_transpose_8x8(const int16x8x4_t* cp, const int16x8x4_t* dp) __attribute__ ((__always_inline__));
+
+inline static int16_8x8_t dq_transpose_8x8(const int16x8x4_t* cp, const int16x8x4_t* dp)
+{
+  int16_8x8_t coef;
+
+  {
+    int16_8x8_t dct;
+    int16x8x2_t unzipped;
+    const int16x8x4_t* coefp = cp;
+    const int16x8x4_t* dctp = dp;
+
+    coef.half[0] = vld4q_s16((const int16_t*) (coefp++));
+    dct.half[0] = vld4q_s16((const int16_t*) (dctp++));
+    coef.half[1] = vld4q_s16((const int16_t*) (coefp++));
+    dct.half[1] = vld4q_s16((const int16_t*) (dctp++));
+
+    coef.half[0].val[0] = vmulq_s16(coef.half[0].val[0], dct.half[0].val[0]);
+    coef.half[0].val[1] = vmulq_s16(coef.half[0].val[1], dct.half[0].val[1]);
+    coef.half[0].val[2] = vmulq_s16(coef.half[0].val[2], dct.half[0].val[2]);
+    coef.half[0].val[3] = vmulq_s16(coef.half[0].val[3], dct.half[0].val[3]);
+
+    coef.half[1].val[0] = vmulq_s16(coef.half[1].val[0], dct.half[1].val[0]);
+    coef.half[1].val[1] = vmulq_s16(coef.half[1].val[1], dct.half[1].val[1]);
+    coef.half[1].val[2] = vmulq_s16(coef.half[1].val[2], dct.half[1].val[2]);
+    coef.half[1].val[3] = vmulq_s16(coef.half[1].val[3], dct.half[1].val[3]);
+
+    unzipped = vuzpq_s16(coef.half[0].val[0], coef.half[1].val[0]);
+    coef.half[0].val[0] = unzipped.val[0];
+    coef.half[1].val[0] = unzipped.val[1];
+    unzipped = vuzpq_s16(coef.half[0].val[1], coef.half[1].val[1]);
+    coef.half[0].val[1] = unzipped.val[0];
+    coef.half[1].val[1] = unzipped.val[1];
+    unzipped = vuzpq_s16(coef.half[0].val[2], coef.half[1].val[2]);
+    coef.half[0].val[2] = unzipped.val[0];
+    coef.half[1].val[2] = unzipped.val[1];
+    unzipped = vuzpq_s16(coef.half[0].val[3], coef.half[1].val[3]);
+    coef.half[0].val[3] = unzipped.val[0];
+    coef.half[1].val[3] = unzipped.val[1];
+
+#ifdef DEBUG_IDCT_DEQUANTIZE
+    fprintf(stderr, "Dequantizing and transposing:\n");
+    const int16x8_t* coeflp = (const int16x8_t*) cp;
+    int i, j;
+    for (i = 0; i < 2; ++i) {
+      for (j = 0; j < 4; ++j) {
+        int16x8_t coef_line = vld1q_s16((const int16_t*) (coeflp++));
+        fprintf(stderr, "   %6d %6d %6d %6d %6d %6d %6d %6d\n",
+                vgetq_lane_s16(coef_line, 0),
+                vgetq_lane_s16(coef_line, 1),
+                vgetq_lane_s16(coef_line, 2),
+                vgetq_lane_s16(coef_line, 3),
+                vgetq_lane_s16(coef_line, 4),
+                vgetq_lane_s16(coef_line, 5),
+                vgetq_lane_s16(coef_line, 6),
+                vgetq_lane_s16(coef_line, 7));
+      }
+    }
+    fprintf(stderr, "using table:\n");
+    coeflp = (const int16x8_t*) dp;
+    for (i = 0; i < 2; ++i) {
+      for (j = 0; j < 4; ++j) {
+        int16x8_t coef_line = vld1q_s16((const int16_t*) (coeflp++));
+        fprintf(stderr, "   %6d %6d %6d %6d %6d %6d %6d %6d\n",
+                vgetq_lane_s16(coef_line, 0),
+                vgetq_lane_s16(coef_line, 1),
+                vgetq_lane_s16(coef_line, 2),
+                vgetq_lane_s16(coef_line, 3),
+                vgetq_lane_s16(coef_line, 4),
+                vgetq_lane_s16(coef_line, 5),
+                vgetq_lane_s16(coef_line, 6),
+                vgetq_lane_s16(coef_line, 7));
+      }
+    }
+    fprintf(stderr, "becomes:\n");
+    for (i = 0; i < 2; ++i) {
+      for (j = 0; j < 4; ++j) {
+        fprintf(stderr, "   %6d %6d %6d %6d %6d %6d %6d %6d\n",
+                vgetq_lane_s16(coef.half[i].val[j], 0),
+                vgetq_lane_s16(coef.half[i].val[j], 1),
+                vgetq_lane_s16(coef.half[i].val[j], 2),
+                vgetq_lane_s16(coef.half[i].val[j], 3),
+                vgetq_lane_s16(coef.half[i].val[j], 4),
+                vgetq_lane_s16(coef.half[i].val[j], 5),
+                vgetq_lane_s16(coef.half[i].val[j], 6),
+                vgetq_lane_s16(coef.half[i].val[j], 7));
+      }
+    }
+    fprintf(stderr, "(dequantized and transposed).\n");
+#endif
+  }
+
+  return coef;
+}
+
+inline static int16_8x8_t idct_helper_8x8(int16x4_t idct_constants, int16_8x8_t a) __attribute__ ((__always_inline__));
+
+inline static int16_8x8_t idct_helper_8x8(int16x4_t idct_constants, int16_8x8_t a)
+{
+  int16x8x4_t even_part, odd_part;
+
+#ifdef DEBUG_IDCT_TRANSFORM
+  int i, j;
+
+  fprintf(stderr, "1-D IDCT of:\n");
+  for (i = 0; i < 2; ++i) {
+    for (j = 0; j < 4; ++j) {
+      fprintf(stderr, "   %6d %6d %6d %6d %6d %6d %6d %6d\n",
+              vgetq_lane_s16(a.half[i].val[j], 0),
+              vgetq_lane_s16(a.half[i].val[j], 1),
+              vgetq_lane_s16(a.half[i].val[j], 2),
+              vgetq_lane_s16(a.half[i].val[j], 3),
+              vgetq_lane_s16(a.half[i].val[j], 4),
+              vgetq_lane_s16(a.half[i].val[j], 5),
+              vgetq_lane_s16(a.half[i].val[j], 6),
+              vgetq_lane_s16(a.half[i].val[j], 7));
+    }
+  }
+  fprintf(stderr, "becomes:\n");
+#endif
+
+  {
+    int16x8_t sum0 = vaddq_s16(a.half[0].val[0], a.half[1].val[0]);
+    int16x8_t sum2 = vaddq_s16(a.half[0].val[2], a.half[1].val[2]);
+    int16x8_t diff0 = vsubq_s16(a.half[0].val[0], a.half[1].val[0]);
+    int16x8_t diff2 = vsubq_s16(a.half[0].val[2], a.half[1].val[2]);
+
+    int16x8_t even_a = vqrdmulhq_lane_s16(diff2, idct_constants, 0);
+    even_a = vaddq_s16(even_a, diff2);
+    even_a = vsubq_s16(even_a, sum2);
+
+    even_part.val[0] = vaddq_s16(sum0, sum2);
+    even_part.val[1] = vaddq_s16(diff0, even_a);
+    even_part.val[2] = vsubq_s16(diff0, even_a);
+    even_part.val[3] = vsubq_s16(sum0, sum2);
+  }
+
+  {
+    int16x8_t sum1 = vaddq_s16(a.half[0].val[1], a.half[1].val[3]);
+    int16x8_t sum3 = vaddq_s16(a.half[1].val[1], a.half[0].val[3]);
+    int16x8_t diff1 = vsubq_s16(a.half[0].val[1], a.half[1].val[3]);
+    int16x8_t diff3 = vsubq_s16(a.half[1].val[1], a.half[0].val[3]);
+
+    int16x8_t t1 = vsubq_s16(sum1, sum3);
+    int16x8_t odd_b = vqrdmulhq_lane_s16(t1, idct_constants, 0);
+    odd_b = vaddq_s16(odd_b, t1);
+
+    t1 = vaddq_s16(diff1, diff3);
+    int16x8_t odd_d = vqrdmulhq_lane_s16(t1, idct_constants, 1);
+    odd_d = vaddq_s16(odd_d, t1);
+
+    int16x8_t odd_a = vqrdmulhq_lane_s16(diff3, idct_constants, 3);
+    odd_a = vaddq_s16(odd_a, diff3);
+    odd_a = vaddq_s16(odd_a, diff3);
+    odd_a = vsubq_s16(odd_d, odd_a);
+
+    int16x8_t odd_c = vqrdmulhq_lane_s16(diff1, idct_constants, 2);
+    odd_c = vaddq_s16(odd_c, diff1);
+    odd_c = vsubq_s16(odd_d, odd_c);
+
+    odd_part.val[0] = vaddq_s16(sum1, sum3);
+    odd_part.val[1] = vsubq_s16(odd_a, odd_part.val[0]);
+    odd_part.val[2] = vsubq_s16(odd_b, odd_part.val[1]);
+    odd_part.val[3] = vsubq_s16(odd_c, odd_part.val[2]);
+  }
+
+  int16_8x8_t b;
+  b.half[0].val[0] = vaddq_s16(even_part.val[0], odd_part.val[0]);
+  b.half[1].val[3] = vsubq_s16(even_part.val[0], odd_part.val[0]);
+  b.half[0].val[1] = vaddq_s16(even_part.val[1], odd_part.val[1]);
+  b.half[1].val[2] = vsubq_s16(even_part.val[1], odd_part.val[1]);
+  b.half[0].val[2] = vaddq_s16(even_part.val[2], odd_part.val[2]);
+  b.half[1].val[1] = vsubq_s16(even_part.val[2], odd_part.val[2]);
+  b.half[0].val[3] = vaddq_s16(even_part.val[3], odd_part.val[3]);
+  b.half[1].val[0] = vsubq_s16(even_part.val[3], odd_part.val[3]);
+
+#ifdef DEBUG_IDCT_TRANSFORM
+  for (i = 0; i < 2; ++i) {
+    for (j = 0; j < 4; ++j) {
+      fprintf(stderr, "   %6d %6d %6d %6d %6d %6d %6d %6d\n",
+              vgetq_lane_s16(b.half[i].val[j], 0),
+              vgetq_lane_s16(b.half[i].val[j], 1),
+              vgetq_lane_s16(b.half[i].val[j], 2),
+              vgetq_lane_s16(b.half[i].val[j], 3),
+              vgetq_lane_s16(b.half[i].val[j], 4),
+              vgetq_lane_s16(b.half[i].val[j], 5),
+              vgetq_lane_s16(b.half[i].val[j], 6),
+              vgetq_lane_s16(b.half[i].val[j], 7));
+    }
+  }
+  fprintf(stderr, "(transformed).\n");
+#endif
+
+  return b;
+}
+
+inline static int16x8x2_t vswp_cheat(uint32x4_t a, uint32x4_t b) __attribute__ ((__always_inline__));
+
+inline static int16x8x2_t vswp_cheat(uint32x4_t a, uint32x4_t b)
+{
+  int16x8x2_t r;
+  asm ( "vswp %f0, %e1 \n\t" : "+w" (a), "+w" (b) : );
+  r.val[0] = vreinterpretq_s16_u32(a);
+  r.val[1] = vreinterpretq_s16_u32(b);
+  return r;
+}
+
+inline static int16_8x8_t transpose_8x8(int16_8x8_t a) __attribute__ ((__always_inline__));
+
+inline static int16_8x8_t transpose_8x8(int16_8x8_t a)
+{
+#ifdef DEBUG_IDCT_TRANSPOSE
+  int i, j;
+
+  fprintf(stderr, "Transposing:\n");
+  for (i = 0; i < 2; ++i) {
+    for (j = 0; j < 4; ++j) {
+      fprintf(stderr, "   %6d %6d %6d %6d %6d %6d %6d %6d\n",
+              vgetq_lane_s16(a.half[i].val[j], 0),
+              vgetq_lane_s16(a.half[i].val[j], 1),
+              vgetq_lane_s16(a.half[i].val[j], 2),
+              vgetq_lane_s16(a.half[i].val[j], 3),
+              vgetq_lane_s16(a.half[i].val[j], 4),
+              vgetq_lane_s16(a.half[i].val[j], 5),
+              vgetq_lane_s16(a.half[i].val[j], 6),
+              vgetq_lane_s16(a.half[i].val[j], 7));
+    }
+  }
+  fprintf(stderr, "becomes:\n");
+#endif
+
+  int16x8x2_t t0 = vtrnq_s16(a.half[0].val[0], a.half[0].val[1]);
+  int16x8x2_t t1 = vtrnq_s16(a.half[0].val[2], a.half[0].val[3]);
+  int16x8x2_t t2 = vtrnq_s16(a.half[1].val[0], a.half[1].val[1]);
+  int16x8x2_t t3 = vtrnq_s16(a.half[1].val[2], a.half[1].val[3]);
+
+  uint32x4x2_t u0 = vtrnq_u32(vreinterpretq_u32_s16(t0.val[0]), vreinterpretq_u32_s16(t1.val[0]));
+  uint32x4x2_t u1 = vtrnq_u32(vreinterpretq_u32_s16(t0.val[1]), vreinterpretq_u32_s16(t1.val[1]));
+  uint32x4x2_t u2 = vtrnq_u32(vreinterpretq_u32_s16(t2.val[0]), vreinterpretq_u32_s16(t3.val[0]));
+  uint32x4x2_t u3 = vtrnq_u32(vreinterpretq_u32_s16(t2.val[1]), vreinterpretq_u32_s16(t3.val[1]));
+
+  int16_8x8_t b;
+  int16x8x2_t r;
+  r = vswp_cheat(u0.val[0], u2.val[0]);
+  b.half[0].val[0] = r.val[0];
+  b.half[1].val[0] = r.val[1];
+  r = vswp_cheat(u1.val[0], u3.val[0]);
+  b.half[0].val[1] = r.val[0];
+  b.half[1].val[1] = r.val[1];
+  r = vswp_cheat(u0.val[1], u2.val[1]);
+  b.half[0].val[2] = r.val[0];
+  b.half[1].val[2] = r.val[1];
+  r = vswp_cheat(u1.val[1], u3.val[1]);
+  b.half[0].val[3] = r.val[0];
+  b.half[1].val[3] = r.val[1];
+
+#ifdef DEBUG_IDCT_TRANSPOSE
+  for (i = 0; i < 2; ++i) {
+    for (j = 0; j < 4; ++j) {
+      fprintf(stderr, "   %6d %6d %6d %6d %6d %6d %6d %6d\n",
+              vgetq_lane_s16(b.half[i].val[j], 0),
+              vgetq_lane_s16(b.half[i].val[j], 1),
+              vgetq_lane_s16(b.half[i].val[j], 2),
+              vgetq_lane_s16(b.half[i].val[j], 3),
+              vgetq_lane_s16(b.half[i].val[j], 4),
+              vgetq_lane_s16(b.half[i].val[j], 5),
+              vgetq_lane_s16(b.half[i].val[j], 6),
+              vgetq_lane_s16(b.half[i].val[j], 7));
+    }
+  }
+  fprintf(stderr, "(transposed).\n");
+#endif
+
+  return b;
+}
+
 static unsigned int simd_support = ~0;
 
 #if defined(__linux__) || defined(ANDROID) || defined(__ANDROID__)
@@ -125,7 +412,10 @@ init_simd (void)
   /* Force different settings through environment variables */
   env = getenv("JSIMD_FORCE_ARM_NEON");
   if ((env != NULL) && (strcmp(env, "1") == 0))
-    simd_support &= JSIMD_ARM_NEON;
+    simd_support |= JSIMD_ARM_NEON;
+  env = getenv("JSIMD_FORCE_ARM_NEON_ASM");
+  if ((env != NULL) && (strcmp(env, "1") == 0))
+    simd_support |= JSIMD_ARM_NEON_ASM;
   env = getenv("JSIMD_FORCE_NO_SIMD");
   if ((env != NULL) && (strcmp(env, "1") == 0))
     simd_support = 0;
@@ -452,8 +742,11 @@ jsimd_can_idct_2x2 (void)
   if (sizeof(ISLOW_MULT_TYPE) != 2)
     return 0;
 
-  if ((simd_support & JSIMD_ARM_NEON))
-    return 1;
+  if ((simd_support & JSIMD_ARM_NEON)) {
+    if ((simd_support & JSIMD_ARM_NEON_ASM))
+      return 1;
+    return -1;
+  }
 
   return 0;
 }
@@ -475,8 +768,11 @@ jsimd_can_idct_4x4 (void)
   if (sizeof(ISLOW_MULT_TYPE) != 2)
     return 0;
 
-  if ((simd_support & JSIMD_ARM_NEON))
-    return 1;
+  if ((simd_support & JSIMD_ARM_NEON)) {
+    if ((simd_support & JSIMD_ARM_NEON_ASM))
+      return 1;
+    return -1;
+  }
 
   return 0;
 }
@@ -486,8 +782,70 @@ jsimd_idct_2x2 (j_decompress_ptr cinfo, jpeg_component_info * compptr,
                 JCOEFPTR coef_block, JSAMPARRAY output_buf,
                 JDIMENSION output_col)
 {
-  if ((simd_support & JSIMD_ARM_NEON))
+  if (!(simd_support & JSIMD_ARM_NEON))
+    return;
+
+  if ((simd_support & JSIMD_ARM_NEON_ASM))
+  {
     jsimd_idct_2x2_neon(compptr->dct_table, coef_block, output_buf, output_col);
+    return;
+  }
+
+  int16_8x8_t coef;
+  int16x8_t zero8 = vdupq_n_s16(0);
+
+  {
+    const int16x8x4_t* coefp = (const int16x8x4_t*) coef_block;
+    const int16x8x4_t* dctp = (const int16x8x4_t*) compptr->dct_table;
+    coef = dq_transpose_8x8(coefp, dctp);
+  }
+
+  {
+    static int16_t idct_constants[4] __attribute__ ((aligned (8)))
+      = { 13573, 27779, 2700, 20091 };
+    int16x4_t c = vld1_s16(idct_constants);
+    coef.half[0].val[2] = zero8;
+    coef.half[1].val[0] = zero8;
+    coef.half[1].val[2] = zero8;
+    coef = transpose_8x8(idct_helper_8x8(c, coef));
+    coef.half[0].val[2] = zero8;
+    coef.half[1].val[0] = zero8;
+    coef.half[1].val[2] = zero8;
+    coef = idct_helper_8x8(c, coef);
+  }
+
+  {
+    int32x4_t half0 = vaddq_s32(
+      vpaddlq_s16(vqaddq_s16(coef.half[0].val[0], coef.half[0].val[1])),
+      vpaddlq_s16(vqaddq_s16(coef.half[0].val[2], coef.half[0].val[3])));
+    int32x4_t half1 = vaddq_s32(
+      vpaddlq_s16(vqaddq_s16(coef.half[1].val[0], coef.half[1].val[1])),
+      vpaddlq_s16(vqaddq_s16(coef.half[1].val[2], coef.half[1].val[3])));
+    int16x4_t all = vqrshrn_n_s32(vcombine_s32(
+      vpadd_s32(vget_low_s32(half0), vget_high_s32(half0)),
+      vpadd_s32(vget_low_s32(half1), vget_high_s32(half1))), 9);
+
+    int16x4_t zero4 = vdup_n_s16(0);
+    uint8x8_t result = vreinterpret_u8_s8(vqmovn_s16(vcombine_s16(all, zero4)));
+    uint8x8_t v128 = vdup_n_u8(128);
+    result = vadd_u8(result, v128);
+
+#ifdef DEBUG_IDCT_NARROW
+    fprintf(stderr, "iDCT result:\n");
+    fprintf(stderr, "   %2x %2x\n   %2x %2x\n",
+            vget_lane_u8(result, 0),
+            vget_lane_u8(result, 1),
+            vget_lane_u8(result, 2),
+            vget_lane_u8(result, 3));
+    fprintf(stderr, "(result stored)\n");
+#endif
+
+    uint8_t buf[16];
+    uint8_t* bufp = (uint8_t*) (((int) &buf[8]) & ~7);
+    vst1_u8(bufp, result);
+    *((uint16_t*)(output_buf[0] + output_col)) = *((uint16_t*)&bufp[0]);
+    *((uint16_t*)(output_buf[1] + output_col)) = *((uint16_t*)&bufp[2]);
+  }
 }
 
 GLOBAL(void)
@@ -495,8 +853,78 @@ jsimd_idct_4x4 (j_decompress_ptr cinfo, jpeg_component_info * compptr,
                 JCOEFPTR coef_block, JSAMPARRAY output_buf,
                 JDIMENSION output_col)
 {
-  if ((simd_support & JSIMD_ARM_NEON))
+  if (!(simd_support & JSIMD_ARM_NEON))
+    return;
+
+  if ((simd_support & JSIMD_ARM_NEON_ASM))
+  {
     jsimd_idct_4x4_neon(compptr->dct_table, coef_block, output_buf, output_col);
+    return;
+  }
+
+  int16_8x8_t coef;
+  int16x8_t zero8 = vdupq_n_s16(0);
+
+  {
+    const int16x8x4_t* coefp = (const int16x8x4_t*) coef_block;
+    const int16x8x4_t* dctp = (const int16x8x4_t*) compptr->dct_table;
+    coef = dq_transpose_8x8(coefp, dctp);
+  }
+
+  {
+    static int16_t idct_constants[4] __attribute__ ((aligned (8)))
+      = { 13573, 27779, 2700, 20091 };
+    int16x4_t c = vld1_s16(idct_constants);
+    coef.half[1].val[0] = zero8;
+    coef = transpose_8x8(idct_helper_8x8(c, coef));
+    coef.half[1].val[0] = zero8;
+    coef = idct_helper_8x8(c, coef);
+  }
+
+  {
+    uint8x16_t result;
+    result = vreinterpretq_u8_s8(vcombine_s8(
+      vqmovn_s16(vcombine_s16(
+        vqrshrn_n_s32(vpaddlq_s16(vqaddq_s16(coef.half[0].val[0], coef.half[0].val[1])), 7),
+        vqrshrn_n_s32(vpaddlq_s16(vqaddq_s16(coef.half[0].val[2], coef.half[0].val[3])), 7))),
+      vqmovn_s16(vcombine_s16(
+        vqrshrn_n_s32(vpaddlq_s16(vqaddq_s16(coef.half[1].val[0], coef.half[1].val[1])), 7),
+        vqrshrn_n_s32(vpaddlq_s16(vqaddq_s16(coef.half[1].val[2], coef.half[1].val[3])), 7)))));
+
+    uint8x16_t v128 = vdupq_n_u8(128);
+    result = vaddq_u8(result, v128);
+
+#ifdef DEBUG_IDCT_NARROW
+    fprintf(stderr, "iDCT result:\n");
+    fprintf(stderr, "   %2x %2x %2x %2x\n   %2x %2x %2x %2x\n",
+            vgetq_lane_u8(result, 0),
+            vgetq_lane_u8(result, 1),
+            vgetq_lane_u8(result, 2),
+            vgetq_lane_u8(result, 3),
+            vgetq_lane_u8(result, 4),
+            vgetq_lane_u8(result, 5),
+            vgetq_lane_u8(result, 6),
+            vgetq_lane_u8(result, 7));
+    fprintf(stderr, "   %2x %2x %2x %2x\n   %2x %2x %2x %2x\n",
+            vgetq_lane_u8(result, 8+0),
+            vgetq_lane_u8(result, 8+1),
+            vgetq_lane_u8(result, 8+2),
+            vgetq_lane_u8(result, 8+3),
+            vgetq_lane_u8(result, 8+4),
+            vgetq_lane_u8(result, 8+5),
+            vgetq_lane_u8(result, 8+6),
+            vgetq_lane_u8(result, 8+7));
+    fprintf(stderr, "(result stored)\n");
+#endif
+
+    uint8_t buf[32];
+    uint8_t* bufp = (uint8_t*) (((int) &buf[16]) & ~15);
+    vst1q_u8(bufp, result);
+    *((uint32_t*)(output_buf[0] + output_col)) = *((uint32_t*)&bufp[0]);
+    *((uint32_t*)(output_buf[1] + output_col)) = *((uint32_t*)&bufp[4]);
+    *((uint32_t*)(output_buf[2] + output_col)) = *((uint32_t*)&bufp[8]);
+    *((uint32_t*)(output_buf[3] + output_col)) = *((uint32_t*)&bufp[12]);
+  }
 }
 
 GLOBAL(int)
@@ -552,8 +980,83 @@ jsimd_idct_ifast (j_decompress_ptr cinfo, jpeg_component_info * compptr,
                 JCOEFPTR coef_block, JSAMPARRAY output_buf,
                 JDIMENSION output_col)
 {
-  if ((simd_support & JSIMD_ARM_NEON))
+  if (!(simd_support & JSIMD_ARM_NEON))
+    return;
+
+  if ((simd_support & JSIMD_ARM_NEON_ASM))
+  {
     jsimd_idct_ifast_neon(compptr->dct_table, coef_block, output_buf, output_col);
+    return;
+  }
+
+  int16_8x8_t coef;
+
+  {
+    const int16x8x4_t* coefp = (const int16x8x4_t*) coef_block;
+    const int16x8x4_t* dctp = (const int16x8x4_t*) compptr->dct_table;
+    coef = dq_transpose_8x8(coefp, dctp);
+  }
+
+  {
+    static int16_t idct_constants[4] __attribute__ ((aligned (8)))
+      = { 13573, 27779, 2700, 20091 };
+    int16x4_t c = vld1_s16(idct_constants);
+    coef = idct_helper_8x8(c, transpose_8x8(idct_helper_8x8(c, coef)));
+  }
+
+  {
+    uint8x16x4_t result;
+    result.val[0] = vreinterpretq_u8_s8(vcombine_s8(
+      vqrshrn_n_s16(coef.half[0].val[0], 5), vqrshrn_n_s16(coef.half[0].val[1], 5)));
+    result.val[1] = vreinterpretq_u8_s8(vcombine_s8(
+      vqrshrn_n_s16(coef.half[0].val[2], 5), vqrshrn_n_s16(coef.half[0].val[3], 5)));
+    result.val[2] = vreinterpretq_u8_s8(vcombine_s8(
+      vqrshrn_n_s16(coef.half[1].val[0], 5), vqrshrn_n_s16(coef.half[1].val[1], 5)));
+    result.val[3] = vreinterpretq_u8_s8(vcombine_s8(
+      vqrshrn_n_s16(coef.half[1].val[2], 5), vqrshrn_n_s16(coef.half[1].val[3], 5)));
+
+    uint8x16_t v128 = vdupq_n_u8(128);
+    result.val[0] = vaddq_u8(result.val[0], v128);
+    result.val[1] = vaddq_u8(result.val[1], v128);
+    result.val[2] = vaddq_u8(result.val[2], v128);
+    result.val[3] = vaddq_u8(result.val[3], v128);
+
+#ifdef DEBUG_IDCT_NARROW
+    int i;
+
+    fprintf(stderr, "iDCT result:\n");
+    for (i = 0; i < 4; ++i) {
+      fprintf(stderr, "   %2x %2x %2x %2x %2x %2x %2x %2x\n",
+              vgetq_lane_u8(result.val[i], 0),
+              vgetq_lane_u8(result.val[i], 1),
+              vgetq_lane_u8(result.val[i], 2),
+              vgetq_lane_u8(result.val[i], 3),
+              vgetq_lane_u8(result.val[i], 4),
+              vgetq_lane_u8(result.val[i], 5),
+              vgetq_lane_u8(result.val[i], 6),
+              vgetq_lane_u8(result.val[i], 7));
+      fprintf(stderr, "   %2x %2x %2x %2x %2x %2x %2x %2x\n",
+              vgetq_lane_u8(result.val[i], 8+0),
+              vgetq_lane_u8(result.val[i], 8+1),
+              vgetq_lane_u8(result.val[i], 8+2),
+              vgetq_lane_u8(result.val[i], 8+3),
+              vgetq_lane_u8(result.val[i], 8+4),
+              vgetq_lane_u8(result.val[i], 8+5),
+              vgetq_lane_u8(result.val[i], 8+6),
+              vgetq_lane_u8(result.val[i], 8+7));
+    }
+    fprintf(stderr, "(result stored)\n");
+#endif
+
+    vst1_u8((uint8_t*)(output_buf[0] + output_col), vget_low_u8(result.val[0]));
+    vst1_u8((uint8_t*)(output_buf[1] + output_col), vget_high_u8(result.val[0]));
+    vst1_u8((uint8_t*)(output_buf[2] + output_col), vget_low_u8(result.val[1]));
+    vst1_u8((uint8_t*)(output_buf[3] + output_col), vget_high_u8(result.val[1]));
+    vst1_u8((uint8_t*)(output_buf[4] + output_col), vget_low_u8(result.val[2]));
+    vst1_u8((uint8_t*)(output_buf[5] + output_col), vget_high_u8(result.val[2]));
+    vst1_u8((uint8_t*)(output_buf[6] + output_col), vget_low_u8(result.val[3]));
+    vst1_u8((uint8_t*)(output_buf[7] + output_col), vget_high_u8(result.val[3]));
+  }
 }
 
 GLOBAL(void)
diff --git a/turbojpeg.c b/turbojpeg.c
index c890bc0..97461da 100644
--- a/turbojpeg.c
+++ b/turbojpeg.c
@@ -214,6 +214,7 @@ static void setDecompDefaults(struct jpeg_decompress_struct *dinfo,
 			_throw("Unsupported pixel format");
 		#endif
 	}
+	dinfo->dct_method=JDCT_IFAST;
 }
 
 
-- 
1.7.0.4

